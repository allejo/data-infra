operator: 'operators.PodOperator'
name: 'dbt-run-and-upload-artifacts'
image: 'ghcr.io/cal-itp/data-infra/warehouse:latest'
cmds:
  - python3

arguments:
  - '/app/scripts/run_and_upload.py'
  - '--docs'
  - '--upload'

is_delete_operator_pod: true
get_logs: true

is_gke: true
pod_location: us-west1
cluster_name: data-infra-apps
namespace: airflow-jobs

env_vars:
  BIGQUERY_KEYFILE_LOCATION: /secrets/jobs-data/service_account.json
  DBT_PROJECT_DIR: /app
  DBT_PROFILE_DIR: /app
  DBT_TARGET: prod_service_account

secrets:
  - deploy_type: volume
    deploy_target: /secrets/jobs-data/
    secret: jobs-data
    key: service-account.json

tolerations:
  - key: pod-role
    operator: Equal
    value: computetask
    effect: NoSchedule
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: pod-role
          operator: In
          values:
          - computetask

dependencies:
  - create_validation_params
