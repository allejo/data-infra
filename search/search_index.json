{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 This website provides technical documentation for the Juniper codebase. Documentation for the main branch is available online at: https://docs.calitp.org/data-infra Editing documentation \u00b6 The docs content all lives under docs/ , with some top-level configuration for how the docs website gets built under mkdocs.yml . To add new sections/articles, simply create new directories/files under docs/ in Markdown format. To preview the rendered docs website live while you work, run script/docs-server from your terminal (requires Python 3.5+). Changes to docs will be published to the online docs website automatically after they are merged into the main branch. Documentation features \u00b6 Material for MkDocs: Reference See mkdocs.yml for enabled plugins/features Mermaid Use code fences with mermaid type to render Mermaid diagrams within docs. For example, this markdown: ```mermaid graph LR Start --> Stop ``` Yields this diagram: graph LR Start --> Stop","title":"Welcome"},{"location":"#welcome","text":"This website provides technical documentation for the Juniper codebase. Documentation for the main branch is available online at: https://docs.calitp.org/data-infra","title":"Welcome"},{"location":"#editing-documentation","text":"The docs content all lives under docs/ , with some top-level configuration for how the docs website gets built under mkdocs.yml . To add new sections/articles, simply create new directories/files under docs/ in Markdown format. To preview the rendered docs website live while you work, run script/docs-server from your terminal (requires Python 3.5+). Changes to docs will be published to the online docs website automatically after they are merged into the main branch.","title":"Editing documentation"},{"location":"#documentation-features","text":"Material for MkDocs: Reference See mkdocs.yml for enabled plugins/features Mermaid Use code fences with mermaid type to render Mermaid diagrams within docs. For example, this markdown: ```mermaid graph LR Start --> Stop ``` Yields this diagram: graph LR Start --> Stop","title":"Documentation features"},{"location":"datasets/01_overview/","text":"Overview \u00b6 page description datasets GTFS Schedule GTFS Schedule data for the current day gtfs_schedule , gtfs_schedule_history , gtfs_schedule_type2 MST Payments TODO TODO Transitstacks TODO transitstacks , views.transitstacks Views End-user friendly data for dashboards and metrics E.g. views.validation_* , views.gtfs_schedule_* Querying data \u00b6 Using metabase dashboards \u00b6 Using siuba \u00b6 Siuba is a tool that allows the same analysis code to run on a pandas DataFrame, as well as generate SQL for different databases. It supports most pandas Series methods analysts use. See the siuba docs for more information. The examples below go through the basics of using siuba, collecting a database query to a local DataFrame, and showing SQL queries that siuba code generates. Basic query \u00b6 from calitp.tables import tbl from siuba import _ , filter , count , collect , show_query # query lastest validation notices, then filter for a single gtfs feed, # and then count how often each code occurs ( tbl . views . validation_notices () >> filter ( _ . calitp_itp_id == 10 , _ . calitp_url_number == 0 ) >> count ( _ . code ) ) # Source: lazy query # DB Conn: Engine(bigquery://cal-itp-data-infra/?maximum_bytes_billed=5000000000) # Preview: calitp_itp_id calitp_url_number agency_name 0 256 0 Porterville Transit 1 257 0 PresidiGo 2 259 0 Redding Area Bus Authority 3 4 0 AC Transit 4 260 0 Beach Cities Transit # .. may have more rows Collect query results \u00b6 Note that siuba by default prints out a preview of the SQL query results. In order to fetch the results of the query as a pandas DataFrame, run collect() . tbl_agency_names = tbl . views . gtfs_agency_names () >> collect () # Use pandas .head() method to show first 5 rows of data tbl_agency_names . head () calitp_itp_id calitp_url_number agency_name 0 256 0 Porterville Transit 1 257 0 PresidiGo 2 259 0 Redding Area Bus Authority 3 4 0 AC Transit 4 260 0 Beach Cities Transit Show query SQL \u00b6 While collect() fetches query results, show_query() prints out the SQL code that siuba generates. ( tbl . views . gtfs_agency_names () >> filter ( _ . agency_name . str . contains ( \"Metro\" )) >> show_query ( simplify = True ) ) SELECT ` anon_1 ` . ` calitp_itp_id ` , ` anon_1 ` . ` calitp_url_number ` , ` anon_1 ` . ` agency_name ` FROM ( SELECT calitp_itp_id , calitp_url_number , agency_name FROM ` views . gtfs_agency_names ` ) AS ` anon_1 ` WHERE regexp_contains ( ` anon_1 ` . ` agency_name ` , 'Metro' ) Note that here the pandas Series method str.contains corresponds to regexp_contains in Google BigQuery.","title":"Overview"},{"location":"datasets/01_overview/#overview","text":"page description datasets GTFS Schedule GTFS Schedule data for the current day gtfs_schedule , gtfs_schedule_history , gtfs_schedule_type2 MST Payments TODO TODO Transitstacks TODO transitstacks , views.transitstacks Views End-user friendly data for dashboards and metrics E.g. views.validation_* , views.gtfs_schedule_*","title":"Overview"},{"location":"datasets/01_overview/#querying-data","text":"","title":"Querying data"},{"location":"datasets/01_overview/#using-metabase-dashboards","text":"","title":"Using metabase dashboards"},{"location":"datasets/01_overview/#using-siuba","text":"Siuba is a tool that allows the same analysis code to run on a pandas DataFrame, as well as generate SQL for different databases. It supports most pandas Series methods analysts use. See the siuba docs for more information. The examples below go through the basics of using siuba, collecting a database query to a local DataFrame, and showing SQL queries that siuba code generates.","title":"Using siuba"},{"location":"datasets/01_overview/#basic-query","text":"from calitp.tables import tbl from siuba import _ , filter , count , collect , show_query # query lastest validation notices, then filter for a single gtfs feed, # and then count how often each code occurs ( tbl . views . validation_notices () >> filter ( _ . calitp_itp_id == 10 , _ . calitp_url_number == 0 ) >> count ( _ . code ) ) # Source: lazy query # DB Conn: Engine(bigquery://cal-itp-data-infra/?maximum_bytes_billed=5000000000) # Preview: calitp_itp_id calitp_url_number agency_name 0 256 0 Porterville Transit 1 257 0 PresidiGo 2 259 0 Redding Area Bus Authority 3 4 0 AC Transit 4 260 0 Beach Cities Transit # .. may have more rows","title":"Basic query"},{"location":"datasets/01_overview/#collect-query-results","text":"Note that siuba by default prints out a preview of the SQL query results. In order to fetch the results of the query as a pandas DataFrame, run collect() . tbl_agency_names = tbl . views . gtfs_agency_names () >> collect () # Use pandas .head() method to show first 5 rows of data tbl_agency_names . head () calitp_itp_id calitp_url_number agency_name 0 256 0 Porterville Transit 1 257 0 PresidiGo 2 259 0 Redding Area Bus Authority 3 4 0 AC Transit 4 260 0 Beach Cities Transit","title":"Collect query results"},{"location":"datasets/01_overview/#show-query-sql","text":"While collect() fetches query results, show_query() prints out the SQL code that siuba generates. ( tbl . views . gtfs_agency_names () >> filter ( _ . agency_name . str . contains ( \"Metro\" )) >> show_query ( simplify = True ) ) SELECT ` anon_1 ` . ` calitp_itp_id ` , ` anon_1 ` . ` calitp_url_number ` , ` anon_1 ` . ` agency_name ` FROM ( SELECT calitp_itp_id , calitp_url_number , agency_name FROM ` views . gtfs_agency_names ` ) AS ` anon_1 ` WHERE regexp_contains ( ` anon_1 ` . ` agency_name ` , 'Metro' ) Note that here the pandas Series method str.contains corresponds to regexp_contains in Google BigQuery.","title":"Show query SQL"},{"location":"datasets/gtfs_schedule/","text":"GTFS Schedule \u00b6 Data \u00b6 dataset description gtfs_schedule Latest warehouse data for GTFS Static feeds. See the GTFS static reference . gtfs_schedule_type2 Tables with GTFS Static feeds across history (going back to May 15 th , 2021). These are stored as type 2 slowly changing dimensions. They have calitp_extracted_at , and calitp_deleted_at fields. gtfs_schedule_history External tables with all new feed data across history. Dashboards \u00b6 Maintenance \u00b6 DAGs overview \u00b6 common issues \u00b6 backfilling \u00b6","title":"GTFS Schedule"},{"location":"datasets/gtfs_schedule/#gtfs-schedule","text":"","title":"GTFS Schedule"},{"location":"datasets/gtfs_schedule/#data","text":"dataset description gtfs_schedule Latest warehouse data for GTFS Static feeds. See the GTFS static reference . gtfs_schedule_type2 Tables with GTFS Static feeds across history (going back to May 15 th , 2021). These are stored as type 2 slowly changing dimensions. They have calitp_extracted_at , and calitp_deleted_at fields. gtfs_schedule_history External tables with all new feed data across history.","title":"Data"},{"location":"datasets/gtfs_schedule/#dashboards","text":"","title":"Dashboards"},{"location":"datasets/gtfs_schedule/#maintenance","text":"","title":"Maintenance"},{"location":"datasets/gtfs_schedule/#dags-overview","text":"","title":"DAGs overview"},{"location":"datasets/gtfs_schedule/#common-issues","text":"","title":"common issues"},{"location":"datasets/gtfs_schedule/#backfilling","text":"","title":"backfilling"},{"location":"datasets/mst_payments/","text":"MST Payments \u00b6 Data \u00b6 dataset description Dashboards \u00b6 Maintenance \u00b6 DAGs overview \u00b6","title":"MST Payments"},{"location":"datasets/mst_payments/#mst-payments","text":"","title":"MST Payments"},{"location":"datasets/mst_payments/#data","text":"dataset description","title":"Data"},{"location":"datasets/mst_payments/#dashboards","text":"","title":"Dashboards"},{"location":"datasets/mst_payments/#maintenance","text":"","title":"Maintenance"},{"location":"datasets/mst_payments/#dags-overview","text":"","title":"DAGs overview"},{"location":"datasets/transitstacks/","text":"Transitstacks \u00b6 Data \u00b6 dataset description Dashboards \u00b6 Maintenance \u00b6 DAGs overview \u00b6","title":"Transitstacks"},{"location":"datasets/transitstacks/#transitstacks","text":"","title":"Transitstacks"},{"location":"datasets/transitstacks/#data","text":"dataset description","title":"Data"},{"location":"datasets/transitstacks/#dashboards","text":"","title":"Dashboards"},{"location":"datasets/transitstacks/#maintenance","text":"","title":"Maintenance"},{"location":"datasets/transitstacks/#dags-overview","text":"","title":"DAGs overview"},{"location":"datasets/views/","text":"Views \u00b6 Data \u00b6 dataset description examples views.gtfs_agency_names One row per GTFS Static data feed, with calitp_itp_id , calitp_url_number , and agency_name views.gtfs_schedule_service_daily calendar.txt data from GTFS Static unpacked and joined with calendar_dates.txt to reflect service schedules on a given day. Critically, it uses the data that was current on service_date . For service_date values in the future, uses most recent data in warehouse. views.validation_notices One line per specific validation violation (e.g. each invalid phone number). See validation_code_descriptions for human friendly code labels, and validation_notice_fields for looking up what columns in validation_notices different codes have data for (e.g. the code \"invalid_phone_number\" sets the fieldValue column). Maintenance \u00b6 DAGs overview \u00b6 Views are held in the gtfs_views DAG.","title":"Views"},{"location":"datasets/views/#views","text":"","title":"Views"},{"location":"datasets/views/#data","text":"dataset description examples views.gtfs_agency_names One row per GTFS Static data feed, with calitp_itp_id , calitp_url_number , and agency_name views.gtfs_schedule_service_daily calendar.txt data from GTFS Static unpacked and joined with calendar_dates.txt to reflect service schedules on a given day. Critically, it uses the data that was current on service_date . For service_date values in the future, uses most recent data in warehouse. views.validation_notices One line per specific validation violation (e.g. each invalid phone number). See validation_code_descriptions for human friendly code labels, and validation_notice_fields for looking up what columns in validation_notices different codes have data for (e.g. the code \"invalid_phone_number\" sets the fieldValue column).","title":"Data"},{"location":"datasets/views/#maintenance","text":"","title":"Maintenance"},{"location":"datasets/views/#dags-overview","text":"Views are held in the gtfs_views DAG.","title":"DAGs overview"},{"location":"warehouse/01_agencies/","text":"Agency GTFS Feeds \u00b6 Feed information is stored in airflow/data/agencies.yml . This file is then used by our Airflow data pipeline to download feeds, and load them into the warehouse. Each agency feed entry includes these fields: agency_name : Human friendly name for the agency itp_id : The California Integrated Travel Project ID URLs such as gtfs_schedule_url , and those for realtime feeds. Finding the ITP ID \u00b6 Entries must include the correct ITP ID for an agency. These can be found on the Primary List of California Transit Providers . Including API Keys \u00b6 If the URL for a GTFS feed requires an API key, you can include using these steps: Add a unique name, and the key to this private spreadsheet . Include the API key in your url by using {{ MY_KEY_NAME }} , where MY_KEY_NAME is the unique name used in the spreadsheet. Below is an example entry. ac-transit : agency_name : AC Transit feeds : - gtfs_schedule_url : https://api.actransit.org/transit/gtfs/download?token={{ AC_TRANSIT_API_KEY }} gtfs_rt_vehicle_positions_url : https://api.actransit.org/gtfsrt/vehicles?token={{ AC_TRANSIT_API_KEY }} gtfs_rt_service_alerts_url : https://api.actransit.org/gtfsrt/alerts?token={{ AC_TRANSIT_API_KEY }} gtfs_rt_trip_updates_url : https://api.actransit.org/gtfsrt/tripupdates?token={{ AC_TRANSIT_API_KEY }} itp_id : 4 Deploying to Pipeline \u00b6 Every time the main branch of cal-itp/data-infra is updated on github, this github action swaps in the API keys and pushes to our Cloud Composer\u2019s data bucket.","title":"Agency GTFS Feeds"},{"location":"warehouse/01_agencies/#agency-gtfs-feeds","text":"Feed information is stored in airflow/data/agencies.yml . This file is then used by our Airflow data pipeline to download feeds, and load them into the warehouse. Each agency feed entry includes these fields: agency_name : Human friendly name for the agency itp_id : The California Integrated Travel Project ID URLs such as gtfs_schedule_url , and those for realtime feeds.","title":"Agency GTFS Feeds"},{"location":"warehouse/01_agencies/#finding-the-itp-id","text":"Entries must include the correct ITP ID for an agency. These can be found on the Primary List of California Transit Providers .","title":"Finding the ITP ID"},{"location":"warehouse/01_agencies/#including-api-keys","text":"If the URL for a GTFS feed requires an API key, you can include using these steps: Add a unique name, and the key to this private spreadsheet . Include the API key in your url by using {{ MY_KEY_NAME }} , where MY_KEY_NAME is the unique name used in the spreadsheet. Below is an example entry. ac-transit : agency_name : AC Transit feeds : - gtfs_schedule_url : https://api.actransit.org/transit/gtfs/download?token={{ AC_TRANSIT_API_KEY }} gtfs_rt_vehicle_positions_url : https://api.actransit.org/gtfsrt/vehicles?token={{ AC_TRANSIT_API_KEY }} gtfs_rt_service_alerts_url : https://api.actransit.org/gtfsrt/alerts?token={{ AC_TRANSIT_API_KEY }} gtfs_rt_trip_updates_url : https://api.actransit.org/gtfsrt/tripupdates?token={{ AC_TRANSIT_API_KEY }} itp_id : 4","title":"Including API Keys"},{"location":"warehouse/01_agencies/#deploying-to-pipeline","text":"Every time the main branch of cal-itp/data-infra is updated on github, this github action swaps in the API keys and pushes to our Cloud Composer\u2019s data bucket.","title":"Deploying to Pipeline"},{"location":"warehouse/02_views/","text":"Creating New Tables (Views) \u00b6 operator description example CsvToWarehouseOperator Load a CSV (or google sheet) from a URL transitstacks.fares PythonToWarehouseOperator Load data using python and the calitp library. This is a very flexible approach. It\u2019s useful when you want to do a small amount of processing before loading. views.validation_code_descriptions SqlToWarehouseOperator Create a new table from a SQL query. views.transitstacks SqlQueryOperator Run arbitrary SQL (e.g. CREATE EXTERNAL TABLE ... ) Syntax overview \u00b6 Below is a file similar to the one used to create views.transitstacks . # What operator to use. This determines the options available, and how the # pipeline executes this task. operator : operators.SqlToWarehouseOperator # Name of the table to load into the warehouse dst_table_name : \"views.dim_date\" # Column descriptions to put into warehouse fields : - id : The ID Column - full_date : The full date (e.g. \"2021-01-01\") - year : The year component (e.g. 2021) # Other tasks this depends on. For example, if it uses another SQL view, then # that view should be a dependency dependencies : - warehouse_loaded # The actual SQL code to be run! sql : | # from https://gist.github.com/ewhauser/d7dd635ad2d4b20331c7f18038f04817 SELECT FORMAT_DATE('%F', d) as id, d AS full_date, EXTRACT(YEAR FROM d) AS year, FROM UNNEST(GENERATE_DATE_ARRAY('2001-01-01', '2050-01-01', INTERVAL 1 DAY)) d Testing Changes \u00b6 Walkthrough \u00b6","title":"Creating New Tables (Views)"},{"location":"warehouse/02_views/#creating-new-tables-views","text":"operator description example CsvToWarehouseOperator Load a CSV (or google sheet) from a URL transitstacks.fares PythonToWarehouseOperator Load data using python and the calitp library. This is a very flexible approach. It\u2019s useful when you want to do a small amount of processing before loading. views.validation_code_descriptions SqlToWarehouseOperator Create a new table from a SQL query. views.transitstacks SqlQueryOperator Run arbitrary SQL (e.g. CREATE EXTERNAL TABLE ... )","title":"Creating New Tables (Views)"},{"location":"warehouse/02_views/#syntax-overview","text":"Below is a file similar to the one used to create views.transitstacks . # What operator to use. This determines the options available, and how the # pipeline executes this task. operator : operators.SqlToWarehouseOperator # Name of the table to load into the warehouse dst_table_name : \"views.dim_date\" # Column descriptions to put into warehouse fields : - id : The ID Column - full_date : The full date (e.g. \"2021-01-01\") - year : The year component (e.g. 2021) # Other tasks this depends on. For example, if it uses another SQL view, then # that view should be a dependency dependencies : - warehouse_loaded # The actual SQL code to be run! sql : | # from https://gist.github.com/ewhauser/d7dd635ad2d4b20331c7f18038f04817 SELECT FORMAT_DATE('%F', d) as id, d AS full_date, EXTRACT(YEAR FROM d) AS year, FROM UNNEST(GENERATE_DATE_ARRAY('2001-01-01', '2050-01-01', INTERVAL 1 DAY)) d","title":"Syntax overview"},{"location":"warehouse/02_views/#testing-changes","text":"","title":"Testing Changes"},{"location":"warehouse/02_views/#walkthrough","text":"","title":"Walkthrough"},{"location":"warehouse/03_metrics/","text":"Metric Views \u00b6 A metric table (or view) is a table that calculates meaningful measures across time. It is defined by the three pieces below. The grain : the business process that each row represents. Metric date columns . metric_date : the day the metric describes. metric_period : the period of time the metric spans (e.g. day for a metric that describes data across a single day; week for when metric_date describes data across a week.) Metric value columns Example: views.validation_code_metrics \u00b6 The views.validation_code_metrics table contains a count of each validator code triggered per GTFS Static data feed in the warehouse. component description grain One row per GTFS Static feed (e.g. an agency\u2019s data), per validator code. metric date columns Calculated for every day with daily periods. Note that metric_period is omitted, but should be set to \u201cday\u201d. metric values The column n_notices counts the number of notices per code.","title":"Metric Views"},{"location":"warehouse/03_metrics/#metric-views","text":"A metric table (or view) is a table that calculates meaningful measures across time. It is defined by the three pieces below. The grain : the business process that each row represents. Metric date columns . metric_date : the day the metric describes. metric_period : the period of time the metric spans (e.g. day for a metric that describes data across a single day; week for when metric_date describes data across a week.) Metric value columns","title":"Metric Views"},{"location":"warehouse/03_metrics/#example-viewsvalidation_code_metrics","text":"The views.validation_code_metrics table contains a count of each validator code triggered per GTFS Static data feed in the warehouse. component description grain One row per GTFS Static feed (e.g. an agency\u2019s data), per validator code. metric date columns Calculated for every day with daily periods. Note that metric_period is omitted, but should be set to \u201cday\u201d. metric values The column n_notices counts the number of notices per code.","title":"Example: views.validation_code_metrics"},{"location":"warehouse/04_sql_snippets/","text":"SQL Snippets \u00b6 Fetching historical data for specific dates \u00b6 Some tables in the warehouse\u2013like those in gtfs_schedule_type2 \u2013capture the full history of data, as it existed every day in the past. In order to do this, they don\u2019t save copies of the full data every day, but log changes to the data using a calitp_extracted_at , and calitp_deleted_at column. In order to get the data for a given day, you need to filter to keep data where.. calitp_extracted_at was earlier than or on the target date. calitp_deleted_at is later than the target date. A single date \u00b6 SELECT * FROM ` gtfs_schedule_type2 . feed_info ` WHERE calitp_extracted_at >= \"2021-06-01\" AND COALESCE ( calitp_deleted_at , \"2099-01-01\" ) < \"2021-06-01\" Note that COALESCE lets us fill in NULL deleted at values to be far in the future. This is used because when deleted at is missing, it reflects the most recent data (i.e. data that hasn\u2019t been deleted yet). Because NULL < \"2021-06-01\" is false , we need to fill it in with a far-future date, so it evaluates to true . Multiple dates \u00b6 In order to do it for a range of dates, you can use a JOIN. This is shown below. SELECT * FROM ` gtfs_schedule_type2 . feed_info ` FI JOIN ` views . dim_date ` D ON FI . calitp_extracted_at <= D . full_date AND COALESCE ( FI . calitp_deleted_at , \"2099-01-01\" ) > D . full_date WHERE D . full_date BETWEEN ( \"2021-06-01\" , \"2021-06-07\" )","title":"SQL Snippets"},{"location":"warehouse/04_sql_snippets/#sql-snippets","text":"","title":"SQL Snippets"},{"location":"warehouse/04_sql_snippets/#fetching-historical-data-for-specific-dates","text":"Some tables in the warehouse\u2013like those in gtfs_schedule_type2 \u2013capture the full history of data, as it existed every day in the past. In order to do this, they don\u2019t save copies of the full data every day, but log changes to the data using a calitp_extracted_at , and calitp_deleted_at column. In order to get the data for a given day, you need to filter to keep data where.. calitp_extracted_at was earlier than or on the target date. calitp_deleted_at is later than the target date.","title":"Fetching historical data for specific dates"},{"location":"warehouse/04_sql_snippets/#a-single-date","text":"SELECT * FROM ` gtfs_schedule_type2 . feed_info ` WHERE calitp_extracted_at >= \"2021-06-01\" AND COALESCE ( calitp_deleted_at , \"2099-01-01\" ) < \"2021-06-01\" Note that COALESCE lets us fill in NULL deleted at values to be far in the future. This is used because when deleted at is missing, it reflects the most recent data (i.e. data that hasn\u2019t been deleted yet). Because NULL < \"2021-06-01\" is false , we need to fill it in with a far-future date, so it evaluates to true .","title":"A single date"},{"location":"warehouse/04_sql_snippets/#multiple-dates","text":"In order to do it for a range of dates, you can use a JOIN. This is shown below. SELECT * FROM ` gtfs_schedule_type2 . feed_info ` FI JOIN ` views . dim_date ` D ON FI . calitp_extracted_at <= D . full_date AND COALESCE ( FI . calitp_deleted_at , \"2099-01-01\" ) > D . full_date WHERE D . full_date BETWEEN ( \"2021-06-01\" , \"2021-06-07\" )","title":"Multiple dates"}]}